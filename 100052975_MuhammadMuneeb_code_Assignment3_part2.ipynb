{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Library\n",
    "Kindly change all paths before executing this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read BBC news data and get texts and labels\n",
    "* label 0: 'business'\n",
    "* label 1: 'entertainment'\n",
    "* label 2: 'politics'\n",
    "* label 3: 'sport'\n",
    "* label 4: 'tech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datasets/business\n",
      "/datasets/entertainment\n",
      "/datasets/politics\n",
      "/datasets/sport\n",
      "/datasets/tech\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/datasets/'\n",
    "labels = []\n",
    "texts = []\n",
    "label_count = 0\n",
    "for label_type in ['business', 'entertainment', 'politics', 'sport', 'tech']:\n",
    "    dir_name = os.path.join(data_dir, label_type)\n",
    "    print(dir_name)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        f = open(os.path.join(dir_name, fname), encoding=\"utf8\", errors='ignore')\n",
    "        texts.append(f.read())\n",
    "        f.close()\n",
    "        labels.append(label_count)\n",
    "    label_count = label_count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into training and testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  1780\n",
      "Testing set:  445\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(texts, labels, test_size=0.20, random_state=42)\n",
    "y_train, y_test= np.array(y_train),np.array(y_test)\n",
    "print(\"Training set: \", len(x_train))\n",
    "print(\"Testing set: \",len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are you going to tokenize the text?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.\n",
    "\n",
    "\n",
    "1. Before we start tokenization we must perform some prepreocessing on the text. We will remove stopwords, punctuations, links and numbers from all the documents and it will help to increase the accuracy of model.\n",
    "2. Code given below will remove all unwanted pieces of information and tokenize the text for each document.\n",
    "\n",
    "3. After that we will tokenize text.I will use default tokenizer to for further preprocessing like all words should be lower case, remove spaces.\n",
    "\n",
    "4. Tokenizer will consider words with max frequency which are 1000 in my case. Only top 100 words will be considered from each document. I am considering tokens of one words only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Muhammad\n",
      "[nltk_data]     Muneeb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "def preprocess(text, stem=False):\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Preprocessing for each of the document.\n",
    "for i in range(0, len(x_train)):\n",
    "    x_train[i] = preprocess(x_train[i])\n",
    "for i in range(0, len(x_test)):\n",
    "    x_test[i] = preprocess(x_test[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code if you are testing this code on GPU.\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do we mean by sequence padding? For your analysis, what is the maximum length of padding you used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding sequences is used to ensure that all sequences in a list have the same length. By default this is done by padding 0 in the beginning of each sequence until each sequence has the same length as the longest sequence. \n",
    "\n",
    "In my case padding length is same as the max word length which is 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26649 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100 \n",
    "training_samples = 1500\n",
    "validation_samples = 280 \n",
    "max_words = 1000 \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)# Tokenizer\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train = tokenizer.texts_to_sequences(x_train)# We have to fit training and testing data.\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "datax=x_train\n",
    "datay=y_train\n",
    "\n",
    "x_train = datax[:training_samples]\n",
    "y_train = datay[:training_samples]\n",
    "x_val = datax[training_samples : training_samples + validation_samples]\n",
    "y_val = datay[training_samples : training_samples + validation_samples]\n",
    "\n",
    "\n",
    "y_train=y_train.reshape(-1,1)\n",
    "y_test=y_test.reshape(-1,1)\n",
    "y_val=y_val.reshape(-1,1)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical# One-hot-encoding for labels.\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_val =  to_categorical(y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do we mean by word embeddings, and what are the advantages of using it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are distributed representations of text in an n-dimensional space. These are essential for solving most NLP problems.\n",
    "Advantage\n",
    "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. That you can either train a new embedding or use a pre-trained embedding on your natural language processing task.\n",
    "\n",
    "1. Easy to visualize and find coorelation of words \n",
    "2. Good for numerical calculation\n",
    "3. Compact\n",
    "\n",
    "That there are 3 main algorithms for learning a word embedding from text data.\n",
    "1. Embedding Layer \n",
    "2. Word2Vec (Skip-gram, CBOW)\n",
    "3. GloVe \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# After that we will use embedding.\n",
    "\n",
    "glove_dir = 'D:\\MSSEMESTER3\\Deeplearning\\Assignments\\Assignment3'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'),encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "      \n",
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def plotting(x_test,y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    y_pred = y_pred.reshape(-1,1)\n",
    "    y_test = y_test.reshape(-1,1)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(accuracy_score(y_test, y_pred))    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          100000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 420,197\n",
      "Trainable params: 420,197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.5947 - acc: 0.8060 - val_loss: 0.2508 - val_acc: 0.9250\n",
      "Epoch 2/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0631 - acc: 0.9793 - val_loss: 0.4762 - val_acc: 0.8679\n",
      "Epoch 3/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0252 - acc: 0.9920 - val_loss: 0.2010 - val_acc: 0.9536\n",
      "Epoch 4/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0067 - acc: 0.9973 - val_loss: 0.2700 - val_acc: 0.9250\n",
      "Epoch 5/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.2818 - val_acc: 0.9179\n",
      "Epoch 6/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0068 - acc: 0.9967 - val_loss: 0.2529 - val_acc: 0.9429\n",
      "Epoch 7/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 5.0019e-04 - acc: 1.0000 - val_loss: 0.4666 - val_acc: 0.9036\n",
      "Epoch 8/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0018 - acc: 0.9993 - val_loss: 0.2640 - val_acc: 0.9321\n",
      "Epoch 9/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.3120 - val_acc: 0.9286\n",
      "Epoch 10/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0011 - acc: 0.9993 - val_loss: 0.3724 - val_acc: 0.9214\n",
      "Epoch 11/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 1.6699e-05 - acc: 1.0000 - val_loss: 0.3258 - val_acc: 0.9429\n",
      "Epoch 12/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 6.7397e-04 - acc: 1.0000 - val_loss: 0.3229 - val_acc: 0.9357\n",
      "Epoch 13/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 3.0111e-06 - acc: 1.0000 - val_loss: 0.4865 - val_acc: 0.9107\n",
      "Epoch 14/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 2.6033e-05 - acc: 1.0000 - val_loss: 0.3819 - val_acc: 0.9357\n",
      "Epoch 15/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 4.7604e-08 - acc: 1.0000 - val_loss: 0.3507 - val_acc: 0.9393\n",
      "Epoch 16/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 7.5499e-09 - acc: 1.0000 - val_loss: 0.3383 - val_acc: 0.9393\n",
      "Epoch 17/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 1.2716e-09 - acc: 1.0000 - val_loss: 0.3454 - val_acc: 0.9393\n",
      "Epoch 18/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 7.1526e-10 - acc: 1.0000 - val_loss: 0.3470 - val_acc: 0.9429\n",
      "Epoch 19/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 3.9736e-10 - acc: 1.0000 - val_loss: 0.3563 - val_acc: 0.9393\n",
      "Epoch 20/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.3414 - val_acc: 0.9393\n",
      "Epoch 21/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 1.5895e-10 - acc: 1.0000 - val_loss: 0.3452 - val_acc: 0.9393\n",
      "Epoch 22/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 2.3842e-10 - acc: 1.0000 - val_loss: 0.3542 - val_acc: 0.9393\n",
      "Epoch 23/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.3413 - val_acc: 0.9393\n",
      "Epoch 24/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 7.9473e-11 - acc: 1.0000 - val_loss: 0.3489 - val_acc: 0.9429\n",
      "Epoch 25/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.3531 - val_acc: 0.9429\n",
      "Epoch 26/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.3491 - val_acc: 0.9393\n",
      "Epoch 27/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.3485 - val_acc: 0.9429\n",
      "Epoch 28/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.3499 - val_acc: 0.9429\n",
      "Epoch 29/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.3475 - val_acc: 0.9357\n",
      "Epoch 30/30\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0000e+00 - acc: 1.0000 - val_loss: 0.3424 - val_acc: 0.9429\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4956 - acc: 0.9169\n",
      "[[103   1   8   2   1]\n",
      " [  2  66   0   3   1]\n",
      " [  4   1  69   1   1]\n",
      " [  0   2   1  99   0]\n",
      " [  7   2   0   0  71]]\n",
      "0.9168539325842696\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          100000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 420,197\n",
      "Trainable params: 420,197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 1.0415 - acc: 0.6593 - val_loss: 0.3597 - val_acc: 0.9214\n",
      "Epoch 2/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0749 - acc: 0.9900 - val_loss: 0.1353 - val_acc: 0.9607\n",
      "Epoch 3/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0963 - val_acc: 0.9679\n",
      "Epoch 4/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 7.0827e-05 - acc: 1.0000 - val_loss: 0.0963 - val_acc: 0.9607\n",
      "Epoch 5/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 1.3277e-06 - acc: 1.0000 - val_loss: 0.1082 - val_acc: 0.9679\n",
      "Epoch 6/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 7.8678e-08 - acc: 1.0000 - val_loss: 0.1105 - val_acc: 0.9714\n",
      "Epoch 7/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 2.0822e-08 - acc: 1.0000 - val_loss: 0.1113 - val_acc: 0.9714\n",
      "Epoch 8/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 8.4241e-09 - acc: 1.0000 - val_loss: 0.1118 - val_acc: 0.9679\n",
      "Epoch 9/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 3.8942e-09 - acc: 1.0000 - val_loss: 0.1125 - val_acc: 0.9714\n",
      "Epoch 10/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 2.6226e-09 - acc: 1.0000 - val_loss: 0.1132 - val_acc: 0.9643\n",
      "Epoch 11/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.4305e-09 - acc: 1.0000 - val_loss: 0.1127 - val_acc: 0.9643\n",
      "Epoch 12/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.1126e-09 - acc: 1.0000 - val_loss: 0.1130 - val_acc: 0.9643\n",
      "Epoch 13/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.2716e-09 - acc: 1.0000 - val_loss: 0.1138 - val_acc: 0.9643\n",
      "Epoch 14/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.5895e-09 - acc: 1.0000 - val_loss: 0.1132 - val_acc: 0.9643\n",
      "Epoch 15/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.3510e-09 - acc: 1.0000 - val_loss: 0.1142 - val_acc: 0.9643\n",
      "Epoch 16/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.7484e-09 - acc: 1.0000 - val_loss: 0.1155 - val_acc: 0.9643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.2716e-09 - acc: 1.0000 - val_loss: 0.1159 - val_acc: 0.9643\n",
      "Epoch 18/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.2716e-09 - acc: 1.0000 - val_loss: 0.1162 - val_acc: 0.9643\n",
      "Epoch 19/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.1921e-09 - acc: 1.0000 - val_loss: 0.1157 - val_acc: 0.9643\n",
      "Epoch 20/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.5100e-09 - acc: 1.0000 - val_loss: 0.1159 - val_acc: 0.9643\n",
      "Epoch 21/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 1.2716e-09 - acc: 1.0000 - val_loss: 0.1184 - val_acc: 0.9643\n",
      "Epoch 22/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.2716e-09 - acc: 1.0000 - val_loss: 0.1171 - val_acc: 0.9643\n",
      "Epoch 23/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.8279e-09 - acc: 1.0000 - val_loss: 0.1172 - val_acc: 0.9643\n",
      "Epoch 24/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 1.4305e-09 - acc: 1.0000 - val_loss: 0.1204 - val_acc: 0.9643\n",
      "Epoch 25/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.5100e-09 - acc: 1.0000 - val_loss: 0.1219 - val_acc: 0.9643\n",
      "Epoch 26/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.1126e-09 - acc: 1.0000 - val_loss: 0.1195 - val_acc: 0.9643\n",
      "Epoch 27/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 1.9073e-09 - acc: 1.0000 - val_loss: 0.1198 - val_acc: 0.9643\n",
      "Epoch 28/30\n",
      "150/150 [==============================] - 1s 4ms/step - loss: 7.1526e-10 - acc: 1.0000 - val_loss: 0.1187 - val_acc: 0.9643\n",
      "Epoch 29/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 1.1921e-09 - acc: 1.0000 - val_loss: 0.1204 - val_acc: 0.9643\n",
      "Epoch 30/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 1.6689e-09 - acc: 1.0000 - val_loss: 0.1198 - val_acc: 0.9643\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.2556 - acc: 0.9483\n",
      "[[109   1   4   0   1]\n",
      " [  1  67   1   2   1]\n",
      " [  2   0  73   0   1]\n",
      " [  0   1   0 101   0]\n",
      " [  3   2   2   1  72]]\n",
      "0.9483146067415731\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "##First model Embedding layer is non trainable \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "model.compile(optimizer='RMSprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=10,\n",
    "                    validation_data=(x_val, y_val))\n",
    "model.evaluate(x_test, y_test)\n",
    "plotting(x_test,y_test)\n",
    "\n",
    "##Second model Embedding layer is trainable \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(optimizer='RMSprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=10,\n",
    "                    validation_data=(x_val, y_val))\n",
    "model.evaluate(x_test, y_test)\n",
    "plotting(x_test,y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List three Recurrent Neural Network-specific hyperparameters you used for training your models? What alternative values those parameters can replace with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the hyper parameters that we can change.\n",
    "\n",
    "1. Number of Layers for RNN (It can be changed in my case it is 1)\n",
    "2. Model of RNN\n",
    "3. Batch Size (It can be changed in my case it is 30)\n",
    "4. Sequence Length\n",
    "5. Number of Epochs\n",
    "6. Clip Gradients at Value\n",
    "7. Learning Rate (By default it is 0.002) It can be changed and in my case it is 0.002\n",
    "8. Decay Rate (By default it is 0.97) It can be changed.\n",
    "\n",
    "I am using 3 hyper parameters. Number of layers for every RNN can be changed depending on the complexity of data.\n",
    "Batch size can be 5,10,20.\n",
    "Number of epochs can be changed to improve the accuracy of model on training data. In my case it is 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.9032 - acc: 0.6967 - val_loss: 0.4837 - val_acc: 0.8750\n",
      "Epoch 2/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.3993 - acc: 0.8913 - val_loss: 0.4768 - val_acc: 0.8857\n",
      "Epoch 3/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.3100 - acc: 0.9133 - val_loss: 0.3230 - val_acc: 0.9000\n",
      "Epoch 4/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.2544 - acc: 0.9327 - val_loss: 0.3063 - val_acc: 0.9036\n",
      "Epoch 5/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.2103 - acc: 0.9380 - val_loss: 0.2211 - val_acc: 0.9214\n",
      "Epoch 6/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.1658 - acc: 0.9480 - val_loss: 0.2109 - val_acc: 0.9500\n",
      "Epoch 7/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.1390 - acc: 0.9593 - val_loss: 0.2698 - val_acc: 0.9214\n",
      "Epoch 8/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.1317 - acc: 0.9653 - val_loss: 0.2065 - val_acc: 0.9464\n",
      "Epoch 9/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0960 - acc: 0.9747 - val_loss: 0.3193 - val_acc: 0.9000\n",
      "Epoch 10/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.1059 - acc: 0.9667 - val_loss: 0.2235 - val_acc: 0.9357\n",
      "Epoch 11/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0939 - acc: 0.9700 - val_loss: 0.1732 - val_acc: 0.9464\n",
      "Epoch 12/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0750 - acc: 0.9753 - val_loss: 0.1802 - val_acc: 0.9536\n",
      "Epoch 13/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0706 - acc: 0.9773 - val_loss: 0.1683 - val_acc: 0.9429\n",
      "Epoch 14/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0475 - acc: 0.9867 - val_loss: 0.2462 - val_acc: 0.9393\n",
      "Epoch 15/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0490 - acc: 0.9833 - val_loss: 0.2019 - val_acc: 0.9607\n",
      "Epoch 16/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0447 - acc: 0.9847 - val_loss: 0.1606 - val_acc: 0.9607\n",
      "Epoch 17/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0281 - acc: 0.9920 - val_loss: 0.1462 - val_acc: 0.9679\n",
      "Epoch 18/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0245 - acc: 0.9920 - val_loss: 0.2008 - val_acc: 0.9643\n",
      "Epoch 19/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0204 - acc: 0.9940 - val_loss: 0.1893 - val_acc: 0.9536\n",
      "Epoch 20/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0210 - acc: 0.9940 - val_loss: 0.2870 - val_acc: 0.9536\n",
      "Epoch 21/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0144 - acc: 0.9960 - val_loss: 0.2274 - val_acc: 0.9607\n",
      "Epoch 22/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0128 - acc: 0.9940 - val_loss: 0.2341 - val_acc: 0.9571\n",
      "Epoch 23/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0182 - acc: 0.9947 - val_loss: 0.2670 - val_acc: 0.9679\n",
      "Epoch 24/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0172 - acc: 0.9953 - val_loss: 0.2100 - val_acc: 0.9643\n",
      "Epoch 25/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0229 - acc: 0.9947 - val_loss: 0.2549 - val_acc: 0.9643\n",
      "Epoch 26/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0076 - acc: 0.9973 - val_loss: 0.2738 - val_acc: 0.9643\n",
      "Epoch 27/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0074 - acc: 0.9987 - val_loss: 0.2675 - val_acc: 0.9607\n",
      "Epoch 28/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0108 - acc: 0.9960 - val_loss: 0.2238 - val_acc: 0.9643\n",
      "Epoch 29/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0054 - acc: 0.9973 - val_loss: 0.3172 - val_acc: 0.9571\n",
      "Epoch 30/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.1546 - val_acc: 0.9679\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.2753 - acc: 0.9528\n",
      "[[105   1   6   1   2]\n",
      " [  1  67   0   2   2]\n",
      " [  0   0  74   1   1]\n",
      " [  0   0   0 101   1]\n",
      " [  2   1   0   0  77]]\n",
      "0.952808988764045\n",
      "Epoch 1/30\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 1.1617 - acc: 0.4907 - val_loss: 0.8477 - val_acc: 0.7500\n",
      "Epoch 2/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.6003 - acc: 0.8500 - val_loss: 0.4472 - val_acc: 0.8786\n",
      "Epoch 3/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.2814 - acc: 0.9353 - val_loss: 0.2609 - val_acc: 0.9464\n",
      "Epoch 4/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.1518 - acc: 0.9620 - val_loss: 0.2916 - val_acc: 0.9250\n",
      "Epoch 5/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0878 - acc: 0.9767 - val_loss: 0.1689 - val_acc: 0.9500\n",
      "Epoch 6/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0790 - acc: 0.9807 - val_loss: 0.1678 - val_acc: 0.9571\n",
      "Epoch 7/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0458 - acc: 0.9880 - val_loss: 0.1709 - val_acc: 0.9464\n",
      "Epoch 8/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0294 - acc: 0.9933 - val_loss: 0.2486 - val_acc: 0.9536\n",
      "Epoch 9/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0237 - acc: 0.9940 - val_loss: 0.2636 - val_acc: 0.9357\n",
      "Epoch 10/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0215 - acc: 0.9927 - val_loss: 0.2086 - val_acc: 0.9464\n",
      "Epoch 11/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0260 - acc: 0.9947 - val_loss: 0.2073 - val_acc: 0.9571\n",
      "Epoch 12/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0111 - acc: 0.9967 - val_loss: 0.2514 - val_acc: 0.9500\n",
      "Epoch 13/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0277 - acc: 0.9927 - val_loss: 0.2785 - val_acc: 0.9571\n",
      "Epoch 14/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0121 - acc: 0.9973 - val_loss: 0.3157 - val_acc: 0.9429\n",
      "Epoch 15/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0056 - acc: 0.9973 - val_loss: 0.2477 - val_acc: 0.9643\n",
      "Epoch 16/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0136 - acc: 0.9980 - val_loss: 0.2203 - val_acc: 0.9679\n",
      "Epoch 17/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0211 - acc: 0.9967 - val_loss: 0.3159 - val_acc: 0.9536\n",
      "Epoch 18/30\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.0068 - acc: 0.9987 - val_loss: 0.3296 - val_acc: 0.9536\n",
      "Epoch 19/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0220 - acc: 0.9960 - val_loss: 0.4432 - val_acc: 0.9429\n",
      "Epoch 20/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 1.9366e-04 - acc: 1.0000 - val_loss: 0.3025 - val_acc: 0.9464\n",
      "Epoch 21/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0240 - acc: 0.9973 - val_loss: 0.4273 - val_acc: 0.9500\n",
      "Epoch 22/30\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.0010 - acc: 0.9993 - val_loss: 0.3659 - val_acc: 0.9500\n",
      "Epoch 23/30\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 1.3092e-06 - acc: 1.0000 - val_loss: 0.5010 - val_acc: 0.9571\n",
      "Epoch 24/30\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.0073 - acc: 0.9993 - val_loss: 0.5189 - val_acc: 0.9536\n",
      "Epoch 25/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 1.9487e-07 - acc: 1.0000 - val_loss: 0.5379 - val_acc: 0.9536\n",
      "Epoch 26/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 8.9475e-05 - acc: 1.0000 - val_loss: 0.4695 - val_acc: 0.9571\n",
      "Epoch 27/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 4.9606e-05 - acc: 1.0000 - val_loss: 0.3751 - val_acc: 0.9643\n",
      "Epoch 28/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 8.0188e-08 - acc: 1.0000 - val_loss: 0.4502 - val_acc: 0.9643\n",
      "Epoch 29/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 2.7895e-08 - acc: 1.0000 - val_loss: 0.6237 - val_acc: 0.9536\n",
      "Epoch 30/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0182 - acc: 0.9987 - val_loss: 0.6390 - val_acc: 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 4ms/step - loss: 0.8942 - acc: 0.9371\n",
      "[[107   1   5   0   2]\n",
      " [  1  64   5   1   1]\n",
      " [  2   0  73   0   1]\n",
      " [  1   1   1  99   0]\n",
      " [  2   2   2   0  74]]\n",
      "0.9370786516853933\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "##First model Embedding layer is non trainable \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=10,\n",
    "                    validation_data=(x_val, y_val))\n",
    "model.evaluate(x_test, y_test)\n",
    "plotting(x_test,y_test)\n",
    "\n",
    "##Second model Embedding layer is trainable \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=10,\n",
    "                    validation_data=(x_val, y_val))\n",
    "model.evaluate(x_test, y_test)\n",
    "plotting(x_test,y_test)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 1.1977 - acc: 0.5607 - val_loss: 0.7241 - val_acc: 0.7786\n",
      "Epoch 2/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.4491 - acc: 0.8847 - val_loss: 0.2820 - val_acc: 0.9071\n",
      "Epoch 3/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.1963 - acc: 0.9473 - val_loss: 0.2418 - val_acc: 0.9286\n",
      "Epoch 4/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.1279 - acc: 0.9580 - val_loss: 0.1728 - val_acc: 0.9500\n",
      "Epoch 5/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.1077 - acc: 0.9667 - val_loss: 0.2022 - val_acc: 0.9429\n",
      "Epoch 6/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0874 - acc: 0.9720 - val_loss: 0.1780 - val_acc: 0.9429\n",
      "Epoch 7/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0752 - acc: 0.9767 - val_loss: 0.1529 - val_acc: 0.9571\n",
      "Epoch 8/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0634 - acc: 0.9787 - val_loss: 0.1823 - val_acc: 0.9429\n",
      "Epoch 9/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0488 - acc: 0.9860 - val_loss: 0.1327 - val_acc: 0.9607\n",
      "Epoch 10/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0451 - acc: 0.9840 - val_loss: 0.1974 - val_acc: 0.9464\n",
      "Epoch 11/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0339 - acc: 0.9927 - val_loss: 0.1622 - val_acc: 0.9607\n",
      "Epoch 12/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0337 - acc: 0.9893 - val_loss: 0.1484 - val_acc: 0.9643\n",
      "Epoch 13/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0279 - acc: 0.9893 - val_loss: 0.1389 - val_acc: 0.9607\n",
      "Epoch 14/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0202 - acc: 0.9960 - val_loss: 0.1854 - val_acc: 0.9536\n",
      "Epoch 15/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0171 - acc: 0.9960 - val_loss: 0.1719 - val_acc: 0.9536\n",
      "Epoch 16/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0220 - acc: 0.9933 - val_loss: 0.1985 - val_acc: 0.9607\n",
      "Epoch 17/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0112 - acc: 0.9967 - val_loss: 0.1550 - val_acc: 0.9643\n",
      "Epoch 18/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0092 - acc: 0.9960 - val_loss: 0.1413 - val_acc: 0.9679\n",
      "Epoch 19/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0080 - acc: 0.9980 - val_loss: 0.1929 - val_acc: 0.9679\n",
      "Epoch 20/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0054 - acc: 0.9993 - val_loss: 0.2444 - val_acc: 0.9464\n",
      "Epoch 21/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0122 - acc: 0.9973 - val_loss: 0.1854 - val_acc: 0.9607\n",
      "Epoch 22/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0141 - acc: 0.9960 - val_loss: 0.2274 - val_acc: 0.9536\n",
      "Epoch 23/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0045 - acc: 0.9987 - val_loss: 0.2352 - val_acc: 0.9536\n",
      "Epoch 24/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0026 - acc: 0.9987 - val_loss: 0.3835 - val_acc: 0.9393\n",
      "Epoch 25/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0126 - acc: 0.9973 - val_loss: 0.2570 - val_acc: 0.9571\n",
      "Epoch 26/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0051 - acc: 0.9973 - val_loss: 0.2220 - val_acc: 0.9607\n",
      "Epoch 27/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0048 - acc: 0.9987 - val_loss: 0.2657 - val_acc: 0.9536\n",
      "Epoch 28/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0018 - acc: 0.9987 - val_loss: 0.2018 - val_acc: 0.9643\n",
      "Epoch 29/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0122 - acc: 0.9973 - val_loss: 0.2631 - val_acc: 0.9679\n",
      "Epoch 30/30\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.3388 - val_acc: 0.9607\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.3466 - acc: 0.9596\n",
      "[[105   2   6   0   2]\n",
      " [  0  71   1   0   0]\n",
      " [  0   1  74   0   1]\n",
      " [  0   1   0 101   0]\n",
      " [  3   1   0   0  76]]\n",
      "0.9595505617977528\n",
      "Epoch 1/30\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 1.3950 - acc: 0.3833 - val_loss: 1.1212 - val_acc: 0.4321\n",
      "Epoch 2/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.9015 - acc: 0.6467 - val_loss: 0.9297 - val_acc: 0.6607\n",
      "Epoch 3/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.5701 - acc: 0.8407 - val_loss: 0.7471 - val_acc: 0.7679\n",
      "Epoch 4/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.3554 - acc: 0.9160 - val_loss: 0.5159 - val_acc: 0.8393\n",
      "Epoch 5/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.2108 - acc: 0.9500 - val_loss: 0.3757 - val_acc: 0.8714\n",
      "Epoch 6/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.1481 - acc: 0.9640 - val_loss: 0.3268 - val_acc: 0.9000\n",
      "Epoch 7/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0899 - acc: 0.9753 - val_loss: 0.2860 - val_acc: 0.9179\n",
      "Epoch 8/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0611 - acc: 0.9827 - val_loss: 0.2743 - val_acc: 0.9071\n",
      "Epoch 9/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0395 - acc: 0.9853 - val_loss: 0.2315 - val_acc: 0.9357\n",
      "Epoch 10/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0213 - acc: 0.9953 - val_loss: 0.2262 - val_acc: 0.9429\n",
      "Epoch 11/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0223 - acc: 0.9933 - val_loss: 0.2622 - val_acc: 0.9250\n",
      "Epoch 12/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0127 - acc: 0.9967 - val_loss: 0.2683 - val_acc: 0.9357\n",
      "Epoch 13/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0095 - acc: 0.9980 - val_loss: 0.4479 - val_acc: 0.9036\n",
      "Epoch 14/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0188 - acc: 0.9960 - val_loss: 0.2442 - val_acc: 0.9464\n",
      "Epoch 15/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0152 - acc: 0.9960 - val_loss: 0.2894 - val_acc: 0.9286\n",
      "Epoch 16/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.3004 - val_acc: 0.9250\n",
      "Epoch 17/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0094 - acc: 0.9987 - val_loss: 0.3381 - val_acc: 0.9357\n",
      "Epoch 18/30\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.0094 - acc: 0.9987 - val_loss: 0.3402 - val_acc: 0.9429\n",
      "Epoch 19/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0065 - acc: 0.9987 - val_loss: 0.2913 - val_acc: 0.9464\n",
      "Epoch 20/30\n",
      "150/150 [==============================] - 1s 7ms/step - loss: 0.0074 - acc: 0.9980 - val_loss: 0.3095 - val_acc: 0.9429\n",
      "Epoch 21/30\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0101 - acc: 0.9987 - val_loss: 0.3577 - val_acc: 0.9464\n",
      "Epoch 22/30\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 7.1413e-04 - acc: 0.9993 - val_loss: 0.3199 - val_acc: 0.9500\n",
      "Epoch 23/30\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 6.1185e-06 - acc: 1.0000 - val_loss: 0.3413 - val_acc: 0.9571\n",
      "Epoch 24/30\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0331 - acc: 0.9947 - val_loss: 0.3278 - val_acc: 0.9607\n",
      "Epoch 25/30\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.0075 - acc: 0.9980 - val_loss: 0.2994 - val_acc: 0.9643\n",
      "Epoch 26/30\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 1.5148e-06 - acc: 1.0000 - val_loss: 0.4950 - val_acc: 0.9429\n",
      "Epoch 27/30\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 3.4573e-06 - acc: 1.0000 - val_loss: 0.3348 - val_acc: 0.9536\n",
      "Epoch 28/30\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.0123 - acc: 0.9980 - val_loss: 0.3230 - val_acc: 0.9643\n",
      "Epoch 29/30\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 2.8737e-07 - acc: 1.0000 - val_loss: 0.3539 - val_acc: 0.9607\n",
      "Epoch 30/30\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 3.3840e-06 - acc: 1.0000 - val_loss: 0.3671 - val_acc: 0.9643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 5ms/step - loss: 0.8742 - acc: 0.9191\n",
      "[[103   1   2   3   6]\n",
      " [  0  66   5   0   1]\n",
      " [  7   2  66   0   1]\n",
      " [  1   0   1 100   0]\n",
      " [  2   3   1   0  74]]\n",
      "0.9191011235955057\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "from keras.layers import GRU\n",
    "##First model Embedding layer is non trainable\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "\n",
    "model.add(GRU(32))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=10,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "model.evaluate(x_test, y_test)\n",
    "plotting(x_test,y_test)\n",
    "##Second model Embedding layer is trainable  \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "\n",
    "model.add(GRU(32))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=10,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "model.evaluate(x_test, y_test)\n",
    "plotting(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the three models’ performance in the testing set. Explain your observation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy\n",
    "Non Trainable Embedding\n",
    "1. Fully Connected 92\n",
    "2. LSTM 95\n",
    "3. GRU 95\n",
    "\n",
    "Trainable Embedding\n",
    "\n",
    "4. Fully Connected 94\n",
    "5. LSTM 94\n",
    "6. GRU 91\n",
    "\n",
    "GRU with non trainable embedding layer performs better as compared to other classifiers. Performance of Lstm and Gru is good as compared to fully connected network. This is because we are dealing with text sequence and RNN models perform good with sequence data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU use less training parameters and therefore use less memory, execute faster and train faster than LSTM's whereas LSTM is more accurate on dataset using longer sequence. In our case we do not have long sequences so we can GRU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU has two gates (reset and update gates) whereas an LSTM has three gates (namely input, output and forget gates) which helps lstm to work better on long sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1000) for input Tensor(\"input_7:0\", shape=(None, 1000), dtype=float32), but it was called on an input with incompatible shape (30, 100).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1000) for input Tensor(\"input_7:0\", shape=(None, 1000), dtype=float32), but it was called on an input with incompatible shape (30, 100).\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.8040 - accuracy: 0.7061WARNING:tensorflow:Model was constructed with shape (None, 1000) for input Tensor(\"input_7:0\", shape=(None, 1000), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "50/50 [==============================] - 2s 32ms/step - loss: 0.7944 - accuracy: 0.7107 - val_loss: 0.2083 - val_accuracy: 0.9393\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 1s 28ms/step - loss: 0.1656 - accuracy: 0.9487 - val_loss: 0.5012 - val_accuracy: 0.8393\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 1s 28ms/step - loss: 0.0938 - accuracy: 0.9733 - val_loss: 0.0716 - val_accuracy: 0.9714\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.98 - 1s 27ms/step - loss: 0.0467 - accuracy: 0.9827 - val_loss: 0.1246 - val_accuracy: 0.9607\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 1s 27ms/step - loss: 0.0366 - accuracy: 0.9900 - val_loss: 0.0760 - val_accuracy: 0.9714\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.1938 - accuracy: 0.9551\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1000) for input Tensor(\"input_7:0\", shape=(None, 1000), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n",
      "[[105   4   4   1   1]\n",
      " [  0  70   1   1   0]\n",
      " [  2   0  73   0   1]\n",
      " [  0   0   0 102   0]\n",
      " [  2   2   1   0  75]]\n",
      "0.9550561797752809\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "    \n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    \n",
    "    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "num_heads = 5  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(max_words,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, 26649, embedding_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embedding_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(\"rmsprop\", \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=30, epochs=5, validation_data=(x_val, y_val)\n",
    ")\n",
    "model.evaluate(x_test, y_test)\n",
    "plotting(x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
